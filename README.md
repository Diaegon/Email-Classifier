# Email Classifier

Sistema inteligente de classifica√ß√£o de emails para empresas do setor financeiro. Utiliza APIs de LLM (Large Language Models) para classificar emails como **Produtivo** ou **Improdutivo** com base no contexto corporativo.

## üéØ Caracter√≠sticas

- **Classifica√ß√£o Inteligente**: Usa IA para analisar emails e determinar se s√£o produtivos ou improdutivos
- **M√∫ltiplas APIs**: Suporte para OpenAI, Anthropic Claude, Google Gemini e Ollama
- **Interface Web**: Frontend intuitivo para classifica√ß√£o de emails
- **Upload de Arquivos**: Suporte para arquivos `.txt` e `.pdf`
- **Respostas Sugeridas**: Gera respostas autom√°ticas baseadas na classifica√ß√£o
- **Contexto Empresarial**: Prompt especializado para empresas do setor financeiro

## Estrutura

- `apps/backend`: API FastAPI com integra√ß√£o LLM
- `apps/frontend`: Frontend est√°tico (HTML/CSS/JS) servido pelo FastAPI

## Requisitos

- Python 3.13+
- Poetry

## Setup e execu√ß√£o (dev)

1. Entre na pasta do backend e instale depend√™ncias:

```bash
cd apps/backend
poetry install
```

2. Configure a API LLM no `.env`:

```bash
# Escolha uma das op√ß√µes abaixo:

# Op√ß√£o 1: OpenAI
LLM_PROVIDER=openai
OPENAI_API_KEY=sua_chave_aqui
OPENAI_MODEL=gpt-4o-mini

# Op√ß√£o 2: Anthropic Claude
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sua_chave_aqui
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Op√ß√£o 3: Google Gemini
LLM_PROVIDER=google
GOOGLE_API_KEY=sua_chave_aqui
GOOGLE_MODEL=gemini-1.5-flash

# Op√ß√£o 4: Ollama (local)
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

3. Rode o servidor:

```bash
poetry run uvicorn email_classifier_llm.main:app --reload --host 0.0.0.0 --port 8000
```

4. Acesse o frontend:

- `http://localhost:8000/` ‚Üí p√°gina com formul√°rio para texto e upload `.txt/.pdf`

## Uso da API

`POST /api/classify` (multipart/form-data)

Campos:
- `text` (opcional) ‚Äî texto do email
- `file` (opcional) ‚Äî arquivo `.txt` ou `.pdf`

Resposta (JSON):

```json
{
  "category": "Produtivo" | "Improdutivo",
  "reason": "string",
  "suggested_reply": "string"
}
```

## Testes

```bash
cd apps/backend
poetry run pytest -q
```

## Deploy

### Railway (Recomendado)
1. Conecte seu reposit√≥rio no [Railway](https://railway.app)
2. Configure as vari√°veis de ambiente:
   - `LLM_PROVIDER=google`
   - `GOOGLE_API_KEY=sua_chave_aqui`
   - `GOOGLE_MODEL=gemini-2.0-flash`
3. Deploy autom√°tico ser√° executado

### Outras op√ß√µes
- Backend: Render / Fly.io
- Frontend: j√° √© servido pelo backend. Alternativamente, hospedar `apps/frontend` em Vercel e apontar para a API p√∫blica.

## Vari√°veis suportadas

- `LLM_PROVIDER`: `openai`, `anthropic`, `google`, `ollama`
- `OPENAI_API_KEY`: chave da OpenAI (se `LLM_PROVIDER=openai`)
- `OPENAI_MODEL`: modelo da OpenAI (padr√£o `gpt-4o-mini`)
- `ANTHROPIC_API_KEY`: chave da Anthropic (se `LLM_PROVIDER=anthropic`)
- `ANTHROPIC_MODEL`: modelo da Anthropic (padr√£o `claude-3-haiku-20240307`)
- `GOOGLE_API_KEY`: chave do Google (se `LLM_PROVIDER=google`)
- `GOOGLE_MODEL`: modelo do Google (padr√£o `gemini-1.5-flash`)
- `OLLAMA_BASE_URL`: URL do Ollama (se `LLM_PROVIDER=ollama`)
- `OLLAMA_MODEL`: modelo do Ollama (padr√£o `llama3.2`)
